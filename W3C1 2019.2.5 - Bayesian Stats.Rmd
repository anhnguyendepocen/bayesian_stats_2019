---
title: "W3C1 2019.2.5 - Bayesian Stats"
author: "Michael Weisner"
date: "2/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Probablity with Continuous Random Variables

### Hypergeometric confusion

Great for drawing withour replacement.

## Probability and Cumulative Mass Function

CMF tells you is like PMF over a continuous space.

- $\Pr\left(\left.x\right|\boldsymbol{\theta}\right)$ is a Probability Mass Function (PMF) 
over a discrete $\Omega$ that may depend on some parameter(s) $\boldsymbol{\theta}$ and the 
Cumulative Mass Function (CMF) is 
$\Pr\left(\left.X\leq x\right|\boldsymbol{\theta}\right)=\sum\limits_{i = \min\{\Omega\} }^x\Pr\left(\left.i\right|\boldsymbol{\theta}\right)$
- In the first roll of bowling, some simplification implies 
$\Pr\left(X\leq x\right) = \frac{ -1 + \mathcal{F}_{x+2}}{- 1 + \mathcal{F}_{n+2}}$

```{r}
source("https://tinyurl.com/y93srfmp") # code from week 1 to define F() and Omega
CMF <- function(x, n = 10) return( (- 1 + F(x + 2)) / (- 1 + F(n + 2)) )
round(CMF(Omega), digits = 4)

```

PMF is the rate of change in the CMF


## Cumulative Density Functions

This is for if $\Omega$ is an interval; e.g. $\Omega = \mathbb{R}, \Omega = \mathbb{R}_+,\Omega = (a,b)$ etc  

$\Omega$ has infinite number of points, so $Pr(X=x)$ goes to 0.

This makes continuous variables more difficult, so we have to use going to 0 since we can't add up an infinite number of points. Then we can do some limit stuff with that.

$\Pr\left(X\leq x\right)$ is called the Cumulative Density Function (CDF) from $\Omega$ to 
$\left(0,1\right)$

There is no conceptual difference between a CMF and a CDF except emphasis on whether $\Omega$ is discrete or continuous so we use  $F\left(\left.x\right|\boldsymbol{\theta}\right)$ for both

```{r, echo = FALSE}
curve(plogis(x), from = -4, to = 4, ylab = "F(x) of Standard Logistic")
```

## The Standard Logistic CDF and PDF

- E.g., CDF of the standard logistic distribution over $\Omega = \mathbb{R}$ is $F\left(x\right) = \frac{1}{1+e^{-x}}$
- $\Pr\left(a<X\leq x\right)=F\left(\left.x\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)$

as in the discrete case

- If $x=a+h$, $\frac{F\left(\left.x\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{x-a}=\frac{F\left(\left.a+h\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{h}$
is the slope of a line segment
- If we then let $h\downarrow0$, $\frac{F\left(\left.a+h\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{h}\rightarrow\frac{\partial F\left(\left.a\right|\boldsymbol{\theta}\right)}{\partial a}\equiv f\left(\left.x\right|\boldsymbol{\theta}\right)$ is still the **RATE OF CHANGE** in $F\left(\left.x\right|\boldsymbol{\theta}\right)$ at $x$
- The **derivative of the CDF** $F\left(x\right)$ is called the **Probability Density Function (PDF)** and denoted $f\left(x\right)$, which is always positive because the CDF increases (the rate of change, the slope at the tangent of the point x is the PDF) 
- $f\left(x\right)$ is NOT a probability but is used like a PMF
- What is slope of $F\left(x\right) = \frac{1}{1 + e^{-x}}$ at $x$?
- [Answer](https://www.wolframalpha.com/input/?i=partial+derivative):
  $\frac{\partial}{\partial x}F\left(x\right) = 
  \frac{-1}{\left(1+e^{-x}\right)^2} \times \frac{\partial}{\partial x}e^{-x} = 
  \frac{-e^{-x}}{\left(1+e^{-x}\right)^2} \times \frac{\partial -x}{\partial x} = 
  \frac{e^{-x}}{\left(1+e^{-x}\right)^2} \geq 0$

## Standard Normal CDF and its Slope

 $\Omega = \mathbb{R}$ is
$$
\Phi(x) = \frac{1}{2} + \phi\left(x\right) S\left(x\right)
$$
where $\phi\left(x\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$,
$S\left(x\right) = \sum_{n=0}^\infty \frac{x^{2n+1}}{\left(2n + 1\right)!!}$, and
$a!!$ is the "double factorial" function for a non-negative integer $a$ such that 
$0!! = 1$, $1!! = 1$, and else $a!! = a \times \left(a - 2\right)!!$. 

What is the slope of $\Phi\left(x\right)$ at $x$?

$$
\frac{\partial}{\partial x}\Phi\left(x\right) = \phi\left(x\right) S^\prime\left(x\right) + 
\phi^\prime\left(x\right) S\left(x\right) = 
\phi\left(x\right)\sum_{n=0}^\infty \frac{\left(2n + 1\right)x^{2n}}{\left(2n + 1\right)!!} -\\
\phi\left(x\right)x\sum_{n=0}^\infty \frac{x^{2n+1}}{\left(2n + 1\right)!!} =
\phi\left(x\right)\sum_{n=0}^\infty \frac{\left(2n + 1\right)x^{2n} - x^{2n+2}}{\left(2n + 1\right)!!} =\\
\phi\left(x\right)
\left(\frac{1 - x^2}{1}+\frac{3x^2 - x^4}{3 \times 1} + \frac{5x^4 - x^6}{5\times3\times1} \dots\right) = 
\phi\left(x\right)
$$

derivative of 1/2 with respect to x is 0
Rule for differentiating the product of 2 functions with respect to x
derivative of $\phi$ is ... $\phi(x)$?
derivative of $e^x$ is $e^x$

### TAKEAWAY
Get PDF from CDF via differentiation and simplification

## CDF and PDF of the Standard Normal Distribution

```{r, fig.height=4.5, fig.width=9, small.mar = TRUE}
curve(pnorm(x), from = -3, to = 3, ylim = c(-1,1), ylab = "") # CDF; what is the median?
curve(dnorm(x), add = TRUE, col = "red", lty = "dashed")      # PDF; what is the mode?
curve(x * dnorm(x), add = TRUE, col = 3, lty = "dotted")      # g function being integrated
legend("topleft", legend = c("CDF", "PDF", "xPDF"), col = 1:3, lty = 1:3, bg = "lightgrey")
```

To find median, we look at 0.5 and go over and then down, in this case it's 0. That means half of the sample space is above and below zero in this case.

use `pnorm` and `dnorm` to get the CDF and PDF respectively. Basically, pdf is the slope of the cdf.

Mode is what maximizes the PDF.

The last is any function * the PDF(x) under a standard normal distribution.

## Expectations of Functions of a Continuous RV

- let g(X) be a function of a continuous $X \in \Omega$
- The probability that X is in the interval $[x,x+dx$ is $f(x|\theta)$ where $dx$ is essentially the smallest non-negligible piece of X.
- The expectatin of g(X), if it exists is defined as...

$$
\mathbb{E}g\left(X\right) = 
\int_{\min \Omega}^{\max \Omega} 
g\left(x\right)f\left(\left.x\right|\boldsymbol{\theta}\right)dx = 
G\left(\boldsymbol{\theta}\right)\bigg\rvert_{x = \min \Omega}^{x = \max \Omega}
$$

- Integrals are usually impossible but we can use simulations to approximate them arbitrarily well. Still need to understand conceptually integrals as areas under a curve... Really between curve and the horizontal axis. In the case of xPDF (the green curve) it's 0.
- Mathematica is free
- If $g(X)=X, \mathbb{X}=\mu$ is "the" expectation and if $g(X)=(X-\mu)^2, \mathbb{E}[(X-\mu)^2] = \mathbb{E}[X^2] - \mu^2 = \sigma^2$ is the variance

## Moments of a Standard Normal Distribution

- Note that the Standard Normal PDF only depends on the square of X, so...

$$
\mu = \int_{-\infty}^\infty{x \phi\left(x\right) dx}
      = \int_{-\infty}^0{x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx} + 
        \int_{0}^\infty{x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx} = 0

$$

Basically above proves that both integrals add up to 0...

Now Integral by substitution:

- Let $y = \frac {x^2}{2}$ so that $\sqrt{2ye}=x$ and $dy = xdx$ then...

$$
\sigma^2 = \int_{-\infty}^\infty{\left(x - 0\right)^2 \phi\left(x\right) dx} = 
2\int_{0}^\infty{\color{blue}{x} \frac{1}{\sqrt{2\pi}}e^{-\color{magenta}{\frac{x^2}{2}}} 
\color{red}{xdx}} =\\
\frac{2}{\sqrt{2\pi}}\int_{0}^\infty{\color{blue}{\sqrt{2y}}e^{-\color{magenta}{y}} \color{red}{dy}} =
\frac{2}{\sqrt{\pi}}\int_{0}^\infty{\color{blue}{y}^{\frac{3}{2} - 1}
e^{-\color{magenta}{y}} \color{red}{dy}} = 
\frac{2}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) = 1
$$

- $\Gamma\left(z\right) = \int_{0}^\infty{y^{z-1}e^{-y} dy}$ is a very important special function that is a continuous gerneralization of $(z+1)$! and is implemented as `gamma(z)` in R.

**REMEMBER GAMMA**


## Shift and Scale Transformations...

- If $Z$ is distributed standard normal & $X(Z) = \mu + \sigma Z$ what's the PDF of X?

Functions of RVs are also RVs. If Z is an RV and specifically a stnadard normal... then X also has to be an RV and what is its PDF?

If we wanted to write Z as a function of X for this case... it would be...

$Z(X) = \frac {X - \mu}{\sigma}$ since this is a monotonic transformation.

$$
\Pr\left(X\leq x\right) = \Pr\left(Z\leq z\left(x\right)\right) = \Phi\left(z\left(x\right)\right) \\
\frac{\partial}{\partial{x}}\Phi\left(z\left(x\right)\right) = 
\frac{\partial \Phi\left(z\right)}{\partial{z}} \times \frac{\partial z\left(x\right)}{\partial{x}} = 
\phi\left(z\left(x\right)\right) \frac{1}{\sigma} = 
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}

$$

So how do we get $\phi$?

Chain Rule (seen above)

So the derivative of phi of z of x... we get little phi of z of x times 1/sigma. etc.

So

- $\mathbb{E}X = \mu + \sigma \mathbb{E}Z = \mu$ and
$\mathbb{E}\left[\left(X - \mu\right)^2\right] = \mathbb{E}\left[\left(\sigma Z\right)^2\right] = \sigma^2\mathbb{E}\left[Z^2\right] = \sigma^2$
- Thus, 
$$
f\left(\left.x\right|\mu,\sigma\right) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$ 
is the PDF of the general normal distribution with expectation $\mu$ and standard deviation $\sigma$ as parameters

- The normal distribution is one of several in the location-scale family, where such
  transformations only change the location and scale of the distribution

## Nonlinear byt Monotonic Transformations
- If $Z$ is distributed normal with expectation $\mu$ and standard deviation $\sigma$
  and $X\left(Z\right) = e^Z$, what is the PDF of $X$? Hint: $\Pr\left(X \leq x\right) = \Pr\left(Z \leq z\left(x\right)\right)$
- Answer: Note that $Z\left(X\right) = \ln X$ and $\frac{\partial}{\partial x}z\left(x\right) = \frac{1}{x}$ so

$$
f_X\left(\left.x\right|\mu,\sigma\right) = f_Z\left(\left.z\left(x\right)\right|\mu,\sigma\right) \times \frac{\partial}{\partial x}z\left(x\right) = \frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{\ln\left(x\right)- \mu}{\sigma}\right)^2}
$$ 
is the PDF of the lognormal distribution over $\Omega = \mathbb{R}_+$

- $\mu$ and $\sigma$ are parameters but NOT the expectation and standard deviation of $X$ (rather they're the expectation and standard deviation of $Z$), due to the nonlinearity of the antilog transformation. It can be shown that $\mathbb{E}X = e^{\mu + \frac{1}{2}\sigma^2}$ and $\mathrm{Var}\left(X\right) = \left(-1 + e^{\sigma^2}\right)e^{2\mu + \sigma^2}$.

## Scale Transformations of Exponential Variates

- Standard exponential distribution over $\Omega = \mathbb{R}_+$ has CDF $F\left(x\right) = 1 - e^{-x}$
- Its PDF is obviously $f\left(x\right) = \frac{\partial}{\partial x}F\left(x\right) = e^{-x}$, which
  must integrate to $1$
- $$
\mathbb{E}X = \int_{0}^\infty xe^{-x}dx = -\left(x+1\right)e^{-x} \bigg\rvert_{x = 0}^{x \rightarrow \infty} \rightarrow -\infty e^{-\infty} + e^{0} \rightarrow 1
$$
- What is $\mathrm{Var}\left(X\right)$?
- $$
\int_{0}^\infty \left(x - 1\right)^2 e^{-x}dx = \int_{0}^\infty x^2 e^{-x}dx - 2\int_{0}^\infty xe^{-x}dx + \int_{0}^\infty e^{-x}dx =\\ -\left(x^2 + 2x + 2\right)e^{-x}\bigg \rvert_{x = 0}^{x \rightarrow \infty} - 2 \times 1 + 1 \rightarrow 1
$$
- If $X$ is distributed standard exponential and $Y = \mu X$, what is the PDF of $Y$?
- Answer: $\Pr\left(X\leq x\right) = \Pr\left(Y \leq y\left(x\right)\right)$, so

  $$
  f\left(\left.y\right|\mu\right) = \frac{\partial 1 - e^{-\frac{y}{\mu}}}{\partial y} = \frac{1}{\mu}e^{-\frac{y}{\mu}}
  $$
- You will often see this with the substitution $\lambda = \frac{1}{\mu}$. What are $\mathbb{E}Y$ & $\mathrm{Var}\left(Y\right)$?

## Bivariate Normal Distribution

If $Pr(X \leq x \cap Y \leq y|\theta) = F(x,y|\theta)$ is a bivariate CDF, then the bivariate PDF is the derivative of $F(x,y}\theta)$. This gerneralizes beyond two dimensions. The PDF of the bivariate normal distribution over 
$\Omega = \mathbb{R}^2$ has five parameters:
$$
f\left(\left.x,y\right|\mu_X,\mu_Y,\sigma_X,\sigma_Y,\rho\right) =\\
\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}e^{-\frac{1}{2\left(1-\rho^2\right)}
\left(\left(\frac{x - \mu_X}{\sigma_X}\right)^2 + 
\left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 - 
2\rho\frac{x - \mu_X}{\sigma_X}\frac{y - \mu_Y}{\sigma_Y}\right)} = \\
\frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu_X}{\sigma_X}\right)^2} \times
\frac{1}{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}\sqrt{2\pi}}e^{-\frac{1}{2}
\left(\frac{y - \left(\color{red}{\mu_y + \frac{\sigma_X}{\sigma_Y}\rho\left(x-\mu_x\right)}\right)}
{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}}\right)^2},
$$ 

where the first term is a marginal normal PDF and the second is a conditional normal PDF w/ parameters $\color{red}{\mu = \mu_Y + \frac{\sigma_X}{\sigma_Y}\rho\left(x-\mu_x\right)}$ & $\color{blue}{\sigma = \sigma_Y\sqrt{1-\rho^2}}$.







